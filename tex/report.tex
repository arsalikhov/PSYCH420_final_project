\documentclass{article}
\usepackage{hyperref}

\usepackage[margin = 1in]{geometry}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{apacite}
\usepackage{graphicx} \graphicspath{ {./Images/} }
\usepackage{blindtext}
\usepackage{titling}
\usepackage{setspace} \doublespacing
\usepackage{wrapfig} 
\setlength\parindent{24pt}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}



\title{Spiking Neural Networks}
\author{Arslan Salikhov \\
	\and 
	Erik Caceros \\
	\and
	Brandon Lam \\
	}
\date{\today}



\begin{document}

\begin{titlingpage}
\maketitle
\begin{abstract}
	Use of Deep Neural Network, commonly referred to as
	\emph{deep learning} spiked in recent years and has been used
	as a tool for impressive advancements in the field of 
	\emph{Artificial Intelligence (AI)}
	Spiking Neural Networks draw inspiration from the 
	Purpose of this project is to demonstrate the capabilities of a 
	Spiking Neural Network and compare it to a more conventional 
	Object Recognition Deep Neural Network
	\end{abstract}
\end{titlingpage}


\tableofcontents
\newpage




\section{Architecture of the Spiking Neural Network}

When it comes to building a neural network, one has to establish an
architecture or an arrangement of layers and how they connect with each other.
It is particularly important, when the network increases in complexity. Object
classification + localization being a demanding task for a network to solve,
we have adopted a published and well-tested architecture for the task --
ResNet. The particular architecture we have tried to imitate titled 
\textit{DECOLLE}, was first developed by \citeA{kaiser2018synaptic} and later
enhanced by \citeA{barchid2021deep}. \textit{DECOLLE} Neural Network adopts 
properties of classical ResNet model architecture as well as a pyramidal structure that
was proposed by \citeA{pyramid}. 

\subsection{ResNets}

\begin{wrapfigure}[21]{r}{0.42\textwidth}
\begin{center}
	\scalebox{0.35}{\includegraphics{resnet_comp.png}}
\end{center}
	\caption{Examples of 16-layer VGG, Plain 34-layer, and 34-layer ResNet Network 
	Architectures}
\end{wrapfigure}




To begin with the cornerstone of our architecture -- ResNet, it was named for its property,
of convolutional layers of the network that are residually connected with each other using
shortcut connections (\citeA{resnet}).
According to the authors of the original paper on ResNets,
addition of residual connections improves model's convergence, or ability for
model's loss to move towards a minimum with a decreasing trend. Meanwhile,
ResNets converge faster than their plain counterparts, the complexity of 
ResNets is much lower than complexity of well established Visual Geometry
Group (VGG) Networks given the same depth (3.6 billion FLOPs (Float Point 
Operations) vs. 19.6 billion FLOPs) (\citeA{resnet}). 

\subsection{Comparing Model Performance}
To compare networks' performance one has to understand how it is measured and what
constitutes an accurate model. For the task of object detection/localization accuracy
of the model at predicting class of the object in the image is not enough. Thus,
two additional performance metrics have been adapted : Mean Average Precision (mAP) and
Intersection of Union (IoU). Firstly, Mean Average Precision for the purposes of Object
Detection is well-defined in the paper by \citeA{evaluation}. According to the authors,
mAP is mean of Average Precision of the model for all the classes in the dataset. It is
calculated using the following equation.


\begin{equation}
	\textbf{mAP} = \frac{1}{N} * \sum_{i = 1}^{N} \textbf{AP}_i
\end{equation}

\begingroup
\fontsize{7pt}{9pt}\selectfont
\begin{enumerate}
	\item[]  \begin{center} AP = Average Precision, \end{center}
	\item[] \begin{center}  N = number of classes. \end{center}
\end{enumerate}
\endgroup


Next, IoU is the area of overlap of ground-truth boxes and the model's detected box output,
the higher the IoU the better performance of the model. Intersection of Union can be 
represented as follows:

\begin{equation}
	\textbf{IoU} = |A \cap B| \div  |A \cup B| = |I| \div |U|
\end{equation}

\begingroup
\fontsize{7pt}{9pt}\selectfont
\begin{enumerate}
	\item[]  \begin{center} I = Intersection area, \end{center}
	\item[] \begin{center}  U = Union area. \end{center}
\end{enumerate}
\endgroup


\subsection{Pyramidal Architecture of Neural Network}
% Moreover, ResNets perform better
% than plain Deep Convolutional Networks in terms of accuracy. In order to compare

Pyramidal structure of a Neural Network refers to the shape of the layers as the model
gets deeper. With each layer dimensions of convolutional layers shrink (usually in half).
For example, the first convolutional layer would have input dimension equal to number of
channels in the image (3 channels in case of RGB images vs. 1 for grayscale) and the output
dimension of 512, with next layer input equal to previous layer's output. Second layer
would have the dimension of 256 (half of previous layer's output) and so on (\citeA{pyramid}).

For the purposes of our network, pyramidal structure of neural network is leveraged using
Encoder-Decoder model. When image is passed through the shrinking layers of the Encoder
Pyramid, it loses resolution as well as contextual information, however when the image
leaves the encoder and is located in the hidden state it is fully comprised of
the semantic information about the image (What object is on the picture and where it is
located). Furthermore, in order to localize an object on the image that model has never 
encountered before, it has to be able to place the semantic information in context. 
This is where Decoder comes into play, being the reverse of an Encoder, the convolutional
layers of decoder double in size until desired dimensions. The decoder is residually
connected to encoder, and thus it gains contextual information about the image through
every pass of a layer.

At the end, in order to extract the meaningful operation, last decoder layer is connected
to fully connected linear layers that produce the output predicted class as well as
detected box coordinates.

\newpage
\bibliographystyle{apacite}
\bibliography{report}

\end{document}