@inproceedings{kim2020spiking,
  title={Spiking-yolo: spiking neural network for energy-efficient object detection},
  author={Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={11270--11277},
  year={2020}
}

@inproceedings{barchid2021deep,
  title={Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning},
  author={Barchid, Sami and Mennesson, Jos{\'e} and Dj{\'e}raba, Chaabane},
  booktitle={2021 International Conference on Content-Based Multimedia Indexing (CBMI)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

@inproceedings{COCOdataset,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  address = {Cham},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle = {Computer Vision -- ECCV 2014},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn = {978-3-319-10602-1},
  pages = {740--755},
  publisher = {Springer International Publishing},
  title = {Microsoft COCO: Common Objects in Context},
  year = {2014}
}

@article{spiking,
 author = {Pfeiffer, Michael and Pfeil, Thomas},
 year = {2018},
 month = {10},
 pages = {},
 title = {Deep Learning With Spiking Neurons: Opportunities and Challenges},
 volume = {12},
 journal = {Frontiers in Neuroscience},
 doi = {10.3389/fnins.2018.00774}
}

@article{kaiser2018synaptic,
  title={Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)},
  author={Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  journal={arXiv preprint arXiv:1811.10766},
  year={2018}
}

@article{eshraghian2021training,
   title   =  {Training spiking neural networks using lessons from deep learning},
   author  =  {Eshraghian, Jason K and Ward, Max and Neftci, Emre and Wang, Xinxin
               and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and
              Jeong, Doo Seok and Lu, Wei D},
   journal = {arXiv preprint arXiv:2109.12894},
   year    = {2021}
}

@misc{surrogateGradient,
  doi = {10.48550/ARXIV.1901.09948},
  url = {https://arxiv.org/abs/1901.09948},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  keywords = {Neural and Evolutionary Computing (cs.NE), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {Surrogate Gradient Learning in Spiking Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{pyramid,
  doi = {10.48550/ARXIV.1612.03144},
  
  url = {https://arxiv.org/abs/1612.03144},
  
  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Feature Pyramid Networks for Object Detection},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{evaluation,
author = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
title = {The Pascal Visual Object Classes (VOC) Challenge},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {88},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-009-0275-4},
doi = {10.1007/s11263-009-0275-4},
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
journal = {Int. J. Comput. Vision},
month = {jun},
pages = {303–338},
numpages = {36},
keywords = {Object recognition, Database, Benchmark, Object detection}
}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 

@incollection{dnnDef,
title = {Comparison of Group-Contribution and Machine Learning-based Property Prediction Models with Uncertainty Quantification},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {755-760},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50118-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065501182},
author = {Adem R.N. Aouichaoui and Resul Al and Jens Abildskov and Gürkan Sin},
keywords = {QSPR, Deep-Learning, Property Prediction, Uncertainty Analysis},
abstract = { Abstract
This study demonstrates the development of three modeling approaches for predicting thermo physical property with the ability to quantify the uncertainty in the prediction. The modeling approaches consist of a classical non-linear group-contribution (GC) model (GCM), Gaussian-Process regression (GPR), and a deep neural network (DNN) all applied to the first-order groups defined by Marrero and Gani as the molecular descriptor. The uncertainty was quantified using different methods: linear error propagation using the parameter covariance matrix for the GCM, the inherent uncertainty quantification of GPR models, and using a probabilistic layer able to learn the distribution of model output sin DNN. The models have been applied to the lower flammability limit (LFL) at 298K. The model performance was evaluated using 5 folds cross-validation to ensure the models were exposed to all data and to detect potential overfitting,—a procedure frequently used with in machine learning. The models obtained produce a good fit to the experimental data when applied to all available data with a coefficient of determination (R2) above 0.9 for all models, a maximum mean absolute error of 0.39 [%-vol], and a maximum mean squared error of 0.51.}
}

@article{maaasSnns,
title = {Networks of spiking neurons: The third generation of neural network models},
journal = {Neural Networks},
volume = {10},
number = {9},
pages = {1659-1671},
year = {1997},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(97)00011-7},
url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
author = {Wolfgang Maass},
keywords = {Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds},
abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.}
}