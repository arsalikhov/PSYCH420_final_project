@inproceedings{kim2020spiking,
  title={Spiking-yolo: spiking neural network for energy-efficient object detection},
  author={Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={07},
  pages={11270--11277},
  year={2020}
}

@inproceedings{barchid2021deep,
  title={Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning},
  author={Barchid, Sami and Mennesson, Jos{\'e} and Dj{\'e}raba, Chaabane},
  booktitle={2021 International Conference on Content-Based Multimedia Indexing (CBMI)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

@inproceedings{COCOdataset,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  address = {Cham},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle = {Computer Vision -- ECCV 2014},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  isbn = {978-3-319-10602-1},
  pages = {740--755},
  publisher = {Springer International Publishing},
  title = {Microsoft COCO: Common Objects in Context},
  year = {2014}
}

@article{spiking,
 author = {Pfeiffer, Michael and Pfeil, Thomas},
 year = {2018},
 month = {10},
 pages = {},
 title = {Deep Learning With Spiking Neurons: Opportunities and Challenges},
 volume = {12},
 journal = {Frontiers in Neuroscience},
 doi = {10.3389/fnins.2018.00774}
}

@article{kaiser2018synaptic,
  title={Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)},
  author={Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  journal={arXiv preprint arXiv:1811.10766},
  year={2018}
}

@article{eshraghian2021training,
   title   =  {Training spiking neural networks using lessons from deep learning},
   author  =  {Eshraghian, Jason K and Ward, Max and Neftci, Emre and Wang, Xinxin
               and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and
              Jeong, Doo Seok and Lu, Wei D},
   journal = {arXiv preprint arXiv:2109.12894},
   year    = {2021}
}

@misc{surrogateGradient,
  doi = {10.48550/ARXIV.1901.09948},
  url = {https://arxiv.org/abs/1901.09948},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  keywords = {Neural and Evolutionary Computing (cs.NE), Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},
  title = {Surrogate Gradient Learning in Spiking Neural Networks},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{resnet,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{pyramid,
  doi = {10.48550/ARXIV.1612.03144},
  
  url = {https://arxiv.org/abs/1612.03144},
  
  author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Feature Pyramid Networks for Object Detection},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{evaluation,
author = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
title = {The Pascal Visual Object Classes (VOC) Challenge},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {88},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-009-0275-4},
doi = {10.1007/s11263-009-0275-4},
abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.},
journal = {Int. J. Comput. Vision},
month = {jun},
pages = {303–338},
numpages = {36},
keywords = {Object recognition, Database, Benchmark, Object detection}
}
@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 