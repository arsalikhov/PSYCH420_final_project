{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import MyOwnDataloader\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 12636.061696\n",
      "free     : 11410.80064\n",
      "used     : 1225.261056\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total/1000000}')\n",
    "print(f'free     : {info.free/1000000}')\n",
    "print(f'used     : {info.used/1000000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=9.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=9.60s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataDir='/media/gamedisk/COCO_dataset/'\n",
    "val='val2017'\n",
    "train = 'train2017'\n",
    "\n",
    "val_annFile='{}/annotations/instances_{}.json'.format(dataDir,val)\n",
    "train_annFile='{}/annotations/instances_{}.json'.format(dataDir,train) \n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "classes = {\n",
    "    \"cat\": 1,\n",
    "    \"dog\": 2\n",
    "}\n",
    "\n",
    "\n",
    "coco = COCO(val_annFile)\n",
    "val_loader = MyOwnDataloader(dataDir = dataDir, dataType = val,\n",
    "                     annFile = val_annFile, classes = classes, train_batch_size=batch_size)\n",
    "valid_dl = val_loader.concat_datasets()\n",
    "\n",
    "\n",
    "coco = COCO(train_annFile)\n",
    "train_loader = MyOwnDataloader(dataDir = dataDir, dataType = train,\n",
    "                     annFile = train_annFile, classes = classes, train_batch_size=batch_size)\n",
    "train_dl = val_loader.concat_datasets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # CNNs for rgb images\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5)\n",
    "        self.conv5 = nn.Conv2d(in_channels=48, out_channels=192, kernel_size=5)\n",
    "\n",
    "\n",
    "        # Connecting CNN outputs with Fully Connected layers for classification\n",
    "        self.class_fc1 = nn.Linear(in_features=1728, out_features=240)\n",
    "        self.class_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.class_out = nn.Linear(in_features=120, out_features=2)\n",
    "\n",
    "        # Connecting CNN outputs with Fully Connected layers for bounding box\n",
    "        self.box_fc1 = nn.Linear(in_features=1728, out_features=240)\n",
    "        self.box_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.box_out = nn.Linear(in_features=120, out_features=4)\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv4(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        t = self.conv5(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.avg_pool2d(t, kernel_size=4, stride=2)\n",
    "\n",
    "        t = torch.flatten(t,start_dim=1)\n",
    "        \n",
    "        # t = t.T\n",
    "\n",
    "        class_t = self.class_fc1(t)\n",
    "        class_t = F.relu(class_t)\n",
    "\n",
    "        class_t = self.class_fc2(class_t)\n",
    "        class_t = F.relu(class_t)\n",
    "\n",
    "        class_t = F.softmax(self.class_out(class_t),dim=1)\n",
    "\n",
    "        box_t = self.box_fc1(t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_fc2(box_t)\n",
    "        box_t = F.relu(box_t)\n",
    "\n",
    "        box_t = self.box_out(box_t)\n",
    "        box_t = F.sigmoid(box_t)\n",
    "\n",
    "        return [class_t,box_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return torch.round(preds).argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # Defining the optimizer\n",
    "    optimizer = optim.SGD(model.parameters(),lr = 0.1)\n",
    "    num_of_epochs = 30\n",
    "    epochs = []\n",
    "    losses = []\n",
    "    # Creating a directory for storing models\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        tot_loss = 0\n",
    "        tot_correct = 0\n",
    "        train_start = time.time()\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for batch, (images, annotations) in tqdm(enumerate(train_dl)):\n",
    "\n",
    "        \t# Converting data from cpu to GPU if available to improve speed\n",
    "            imgs = list(img.to(device) for img in images)\n",
    "            neural_images = torch.stack(imgs)\n",
    "            x = neural_images.to(device)\n",
    "            y = annotations\n",
    "            y = y.type(torch.LongTensor)\n",
    "            y = y.to(device)\n",
    "            z = annotations[0]['boxes']\n",
    "            # z = z.type(torch.LongTensor)\n",
    "            z = z.to(device)\n",
    "            # Sets the gradients of all optimized tensors to zero\n",
    "            optimizer.zero_grad()\n",
    "            [y_pred,z_pred]= model(x)\n",
    "            print(y)\n",
    "            # Compute loss (here CrossEntropyLoss)\n",
    "            class_loss = F.cross_entropy(y_pred, y)\n",
    "            box_loss = F.mse_loss(z_pred[0], z)\n",
    "            (box_loss + class_loss).backward()\n",
    "            # class_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train batch:\", batch+1, \" epoch: \", epoch, \" \",\n",
    "                  (time.time()-train_start)/60, end='\\r')\n",
    "\n",
    "        model.eval()\n",
    "        for batch, (images, annotations) in tqdm(enumerate(valid_dl)):\n",
    "        \t# Converting data from cpu to GPU if available to improve speed\t\n",
    "            imgs = list(img.to(device) for img in images)\n",
    "            neural_images = torch.stack(imgs)\n",
    "            x = neural_images.to(device)\n",
    "            annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "            y = annotations[0]['labels'].to(device)\n",
    "            z = annotations[0]['boxes'].to(device)\n",
    "            # Sets the gradients of all optimized tensors to zero\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                [y_pred,z_pred]= model(x)\n",
    "                \n",
    "                # Compute loss (here CrossEntropyLoss)\n",
    "                class_loss = F.cross_entropy(y_pred, y)\n",
    "                box_loss = F.mse_loss(z_pred[0], z)\n",
    "                # Compute loss (here CrossEntropyLoss)\n",
    "\n",
    "            tot_loss += (class_loss.item() + box_loss.item())\n",
    "            tot_correct += get_num_correct(y_pred, y)\n",
    "            print(\"Test batch:\", batch+1, \" epoch: \", epoch, \" \",\n",
    "                  (time.time()-train_start)/60, end='\\r')\n",
    "        epochs.append(epoch)\n",
    "        losses.append(tot_loss)\n",
    "        print(\"Epoch\", epoch, \"Accuracy\", (tot_correct)/2.4, \"loss:\",\n",
    "              tot_loss, \" time: \", (time.time()-train_start)/60, \" mins\")\n",
    "        torch.save(model.state_dict(), \"model_ep\"+str(epoch+1)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv4): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv5): Conv2d(48, 192, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (class_fc1): Linear(in_features=1728, out_features=240, bias=True)\n",
       "  (class_fc2): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (class_out): Linear(in_features=120, out_features=2, bias=True)\n",
       "  (box_fc1): Linear(in_features=1728, out_features=240, bias=True)\n",
       "  (box_fc2): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (box_out): Linear(in_features=120, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final _model.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000010?line=0'>1</a>\u001b[0m train(model)\n",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final _model.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000009?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m neural_images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000009?line=21'>22</a>\u001b[0m y \u001b[39m=\u001b[39m annotations\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000009?line=22'>23</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39mLongTensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000009?line=23'>24</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/non_snn_final%20_model.ipynb#ch0000009?line=24'>25</a>\u001b[0m z \u001b[39m=\u001b[39m annotations[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62d0206b570d77ef1bfa49bb04057592ebbd6080402d9290ba5af22960e1b27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
