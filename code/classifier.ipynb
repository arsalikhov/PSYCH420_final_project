{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import MyOwnDataloader\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, timesteps: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64,\n",
    "                               kernel_size=7,\n",
    "                               padding=3,\n",
    "                               # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                               bias=True,\n",
    "                               dilation=1,\n",
    "                               stride=2)\n",
    "        self.spike1 = snn.Leaky(\n",
    "            beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        # residual block 2\n",
    "        self.conv2 = nn.Conv2d(64, 64,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                               bias=True,\n",
    "                               stride=2)\n",
    "        self.spike2 = snn.Leaky(\n",
    "            beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               bias=True,\n",
    "                               stride=2)\n",
    "        self.spike3 = snn.Leaky(\n",
    "            beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        # residual block 3\n",
    "        self.conv4 = nn.Conv2d(128, 256,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                               bias=True,\n",
    "                               stride=2)\n",
    "        self.spike4 = snn.Leaky(\n",
    "            beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               bias=True,\n",
    "                               stride=2)\n",
    "        self.spike5 = snn.Leaky(\n",
    "            beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        # classifying layers\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((512, 10))\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(512, out_channels, bias=True)\n",
    "        self.fc_spike = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(\n",
    "            slope=25), init_hidden=False, output=True)\n",
    "\n",
    "        # self.final = nn.Linear(128, out_channels, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # resets every LIF neurons\n",
    "        mem_spike1 = self.spike1.init_leaky()\n",
    "        mem_spike2 = self.spike2.init_leaky()\n",
    "        mem_spike3 = self.spike3.init_leaky()\n",
    "        mem_spike4 = self.spike4.init_leaky()\n",
    "        mem_spike5 = self.spike5.init_leaky()\n",
    "\n",
    "        mem_fc_spike = self.fc_spike.init_leaky()\n",
    "\n",
    "        # mem accumulator to get the prediction\n",
    "        accumulator = []\n",
    "\n",
    "        for k in range(self.timesteps):\n",
    "            x = inputs[k, :, :, :]\n",
    "            x = F.max_pool2d(self.conv1(x), 2)\n",
    "            x, mem_spike1 = self.spike1(x, mem_spike1)\n",
    "\n",
    "            x = F.max_pool2d(self.conv2(x), 2)\n",
    "            x, mem_spike2 = self.spike2(x, mem_spike2)\n",
    "\n",
    "            x = F.max_pool2d(self.conv3(x), 2)\n",
    "            x, mem_spike3 = self.spike3(x, mem_spike3)\n",
    "\n",
    "            x = F.max_pool2d(self.conv4(x), 2)\n",
    "            x, mem_spike4 = self.spike4(x, mem_spike4)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x, mem_spike5 = self.spike5(x, mem_spike5)\n",
    "\n",
    "            x = self.avg_pool(x)\n",
    "\n",
    "            # classifier\n",
    "            # x = self.flat(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x, mem_fc_spike = self.fc_spike(x, mem_fc_spike)\n",
    "\n",
    "            x = self.final(x)\n",
    "\n",
    "            accumulator.append(mem_fc_spike)\n",
    "\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataDir='/media/gamedisk/COCO_dataset/'\n",
    "val='val2017'\n",
    "train = 'train2017'\n",
    "\n",
    "val_annFile='{}/annotations/instances_{}.json'.format(dataDir,val)\n",
    "train_annFile='{}/annotations/instances_{}.json'.format(dataDir,train) \n",
    "# Batch size\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "classes = {\n",
    "    \"bird\": 1,\n",
    "    \"cat\": 2,\n",
    "    \"dog\": 3,\n",
    "    \"horse\": 4,\n",
    "    \"sheep\": 5,\n",
    "    \"cow\": 6,\n",
    "    \"elephant\": 7,\n",
    "    \"bear\": 8,\n",
    "    \"zebra\": 9,\n",
    "    \"giraffe\": 10\n",
    "}\n",
    "\n",
    "\n",
    "coco = COCO(val_annFile)\n",
    "val_loader = MyOwnDataloader(dataDir = dataDir, dataType = val,\n",
    "                     annFile = val_annFile, classes = classes, train_batch_size=batch_size, classifier=True)\n",
    "valid_dl = val_loader.concat_datasets()\n",
    "\n",
    "\n",
    "# coco = COCO(train_annFile)\n",
    "# train_loader = MyOwnDataloader(dataDir = dataDir, dataType = train,\n",
    "#                      annFile = train_annFile, classes = classes, train_batch_size=batch_size)\n",
    "# train_dl = val_loader.concat_datasets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(3,10, 100)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = next(iter(valid_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/69 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x10 and 512x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000006?line=4'>5</a>\u001b[0m imgs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000006?line=5'>6</a>\u001b[0m imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(imgs)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000006?line=6'>7</a>\u001b[0m spk_rec, mem_rec \u001b[39m=\u001b[39m net(imgs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000006?line=7'>8</a>\u001b[0m loss_val \u001b[39m=\u001b[39m loss_fn(spk_rec, target)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000006?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(loss_val)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb Cell 3'\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=94'>95</a>\u001b[0m \u001b[39m# classifier\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=95'>96</a>\u001b[0m \u001b[39m# x = self.flat(x)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=96'>97</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=97'>98</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=98'>99</a>\u001b[0m x, mem_fc_spike \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_spike(x, mem_fc_spike)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/classifier.ipynb#ch0000002?line=100'>101</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x10 and 512x10)"
     ]
    }
   ],
   "source": [
    "loss_fn = SF.ce_rate_loss()\n",
    "net.train()\n",
    "for data, target in tqdm(valid_dl):\n",
    "    label= int(target[0]['labels'][0])\n",
    "    imgs = list(img.to(device) for img in data)\n",
    "    imgs = torch.stack(imgs)\n",
    "    spk_rec, mem_rec = net(imgs)\n",
    "    loss_val = loss_fn(spk_rec, target)\n",
    "    print(loss_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = batch_accuracy(test_loader, net, num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "num_epochs = 10\n",
    "test_acc_hist = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    avg_loss = backprop.BPTT(net, train_loader, optimizer=optimizer, criterion=loss_fn,\n",
    "                            num_steps=num_steps, time_var=False, device=device)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Train Loss: {avg_loss.item():.2f}\")\n",
    "\n",
    "    # Test set accuracy\n",
    "    test_acc = batch_accuracy(train_loader, net, num_steps)\n",
    "    test_acc_hist.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Test Acc: {test_acc * 100:.2f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62d0206b570d77ef1bfa49bb04057592ebbd6080402d9290ba5af22960e1b27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
