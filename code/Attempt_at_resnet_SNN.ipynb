{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an attempt to integrate snnTorch into a ResNet written with pytorch. However, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataLoader import MyOwnDataloader\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total/1000000}')\n",
    "print(f'free     : {info.free/1000000}')\n",
    "print(f'used     : {info.used/1000000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "dataDir='/media/gamedisk/COCO_dataset/'\n",
    "val='val2017'\n",
    "train = 'train2017'\n",
    "test = 'test2017'\n",
    "\n",
    "val_annFile='{}/annotations/instances_{}.json'.format(dataDir,val)\n",
    "train_annFile='{}/annotations/instances_{}.json'.format(dataDir,train)\n",
    "test_annFile='/media/gamedisk/COCO_dataset/annotations/image_info_test2017.json' \n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "classes = {\n",
    "    \"bird\": 1,\n",
    "    \"cat\": 2,\n",
    "    \"dog\": 3,\n",
    "    \"horse\": 4,\n",
    "    \"sheep\": 5,\n",
    "    \"cow\": 6,\n",
    "    \"elephant\": 7,\n",
    "    \"bear\": 8,\n",
    "    \"zebra\": 9,\n",
    "    \"giraffe\": 10\n",
    "}\n",
    "\n",
    "\n",
    "train_loader = MyOwnDataloader(dataDir = dataDir, dataType = train,\n",
    "                     annFile = train_annFile, classes = classes, train_batch_size=batch_size)\n",
    "train_dl = train_loader.concat_datasets()\n",
    "\n",
    "\n",
    "# test_loader = MyOwnDataloader(dataDir = dataDir, dataType = test,\n",
    "#                      annFile = test_annFile, classes = classes, train_batch_size=batch_size)\n",
    "# test_dl = test_loader.concat_datasets()\n",
    "\n",
    "\n",
    "val_loader = MyOwnDataloader(dataDir = dataDir, dataType = val,\n",
    "                     annFile = val_annFile, classes = classes, train_batch_size=batch_size)\n",
    "valid_dl = val_loader.concat_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.5\n",
    "num_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, timesteps: int):\n",
    "        super(Network, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        # CNNs for rgb images\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32,\n",
    "                        kernel_size=7,\n",
    "                        padding=3,\n",
    "                        # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                        bias=True,\n",
    "                        dilation=1,\n",
    "                        stride=2)\n",
    "        self.lif1 = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64,\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                               bias=True,\n",
    "                               stride=2)\n",
    "        self.lif2 = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                        bias=True,\n",
    "                        stride=2)\n",
    "        self.lif3 = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256,\n",
    "                        kernel_size=3,\n",
    "                        padding=1,\n",
    "                        # no bias because it is not bio-plausible (and hard to impl in neuromorphic hardware)\n",
    "                        bias=True,\n",
    "                        stride=2)\n",
    "        self.lif4 = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 512,\n",
    "                    kernel_size=3,\n",
    "                    padding=1,\n",
    "                    bias=True,\n",
    "                    stride=2)\n",
    "        self.lif5 = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(slope=25), init_hidden=False)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(512, out_channels, bias=True)\n",
    "        self.fc_spike = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(\n",
    "            slope=25), init_hidden=False, output=True)\n",
    "\n",
    "\n",
    "        # # Connecting CNN outputs with Fully Connected layers for classification\n",
    "\n",
    "        self.class_fc1 = nn.Linear(in_features=192*3*3, out_features=240)\n",
    "        self.class_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.class_out = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "        self.fc_spike = snn.Leaky(beta=0.5, spike_grad=surrogate.fast_sigmoid(\n",
    "            slope=25), init_hidden=False, output=True)\n",
    "\n",
    "        Connecting CNN outputs with Fully Connected layers for bounding box\n",
    "        self.box_fc1 = nn.Linear(in_features=192*3*3, out_features=240)\n",
    "        self.box_fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.box_out = nn.Linear(in_features=120, out_features=4)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "        mem5 = self.lif5.init_leaky()\n",
    "\n",
    "        mem_fc_spike = self.fc_spike.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk5_rec = []\n",
    "        mem5_rec = []\n",
    "\n",
    "        for k in range(self.timesteps):\n",
    "            t = inputs[k, :, :, :]\n",
    "            # t = inputs\n",
    "            t = self.conv1(t)\n",
    "            t = F.max_pool2d(t, kernel_size=2)\n",
    "            t, mem1 = self.lif1(t, mem1)\n",
    "\n",
    "            t = self.conv2(t)\n",
    "            t = F.max_pool2d(t, kernel_size=2,)\n",
    "            t, mem2 = self.lif2(t, mem2)\n",
    "\n",
    "\n",
    "            t = self.conv3(t)\n",
    "            t = F.max_pool2d(t, kernel_size=2)\n",
    "            t, mem3 = self.lif3(t, mem3)\n",
    "\n",
    "\n",
    "            t = self.conv4(t)\n",
    "            t = F.max_pool2d(t, kernel_size=2)\n",
    "            t, mem4 = self.lif4(t, mem4)\n",
    "\n",
    "\n",
    "            t = self.conv5(t)\n",
    "            class_t, mem5 = self.lif5(t, mem5)\n",
    "            # t = F.relu(t)\n",
    "            # t = F.avg_pool2d(t, 1)\n",
    "\n",
    "            class_t = self.avg_pool(class_t)\n",
    "            # t = torch.flatten(t,start_dim=1)\n",
    "\n",
    "            class_t = self.flat(class_t)\n",
    "            class_t = self.dropout(class_t)\n",
    "            class_t = class_t.T\n",
    "            class_t = self.fc(class_t)\n",
    "            class_t, mem_fc_spike = self.fc_spike(class_t, mem_fc_spike)\n",
    "            spk5_rec.append(class_t)\n",
    "            mem5_rec.append(mem_fc_spike)\n",
    "\n",
    "            # classifier\n",
    "            # class_t = self.class_fc1(t)\n",
    "            # class_t = F.relu(class_t)\n",
    "\n",
    "            # class_t = self.class_fc2(class_t)\n",
    "            # class_t = F.relu(class_t)\n",
    "\n",
    "\n",
    "            # class_t, mem_fc_spike = self.fc_spike(class_t, mem_fc_spike)\n",
    "\n",
    "\n",
    "\n",
    "            # box_t = self.box_fc1(t)\n",
    "            # box_t = F.relu(box_t)\n",
    "\n",
    "            # box_t = self.box_fc2(box_t)\n",
    "            # box_t = F.relu(box_t)\n",
    "\n",
    "            # box_t = self.box_out(box_t)\n",
    "            # box_t = F.sigmoid(box_t)\n",
    "\n",
    "        return torch.stack(spk5_rec, dim=0), torch.stack(mem5_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(3, 10, 8)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(net, num_steps, data):\n",
    "  mem_rec = []\n",
    "  spk_rec = []\n",
    "  utils.reset(net)  # resets hidden states for all LIF neurons in net\n",
    "\n",
    "  for step in range(num_steps):\n",
    "      spk_out, mem_out = net(data)\n",
    "      spk_rec.append(spk_out)\n",
    "      mem_rec.append(mem_out)\n",
    "\n",
    "  return torch.stack(spk_rec), torch.stack(mem_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    # train_loader = iter(train_loader)\n",
    "    for batch, (images, annotations) in tqdm(enumerate(train_loader)):\n",
    "      imgs = list(img.to(device) for img in images)\n",
    "      neural_images = torch.stack(imgs)\n",
    "      x = neural_images.to(device)\n",
    "      annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "      y = annotations[0]['labels'].to(device)\n",
    "      z = annotations[0]['boxes'].to(device)\n",
    "      print(len(y))\n",
    "      try:\n",
    "        spk_rec, _ = forward_pass(net, num_steps, x)\n",
    "\n",
    "        acc += SF.accuracy_rate(spk_rec[0], y[0]) * spk_rec.size(1)\n",
    "        total += spk_rec.size(1)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "test_acc = batch_accuracy(valid_dl, model, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total accuracy on the test set is: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = forward_pass(model, num_steps, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = model(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
    "    print_batch_accuracy(data, targets, train=True)\n",
    "    print_batch_accuracy(test_data, test_targets, train=False)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "for batch, (images, annotations) in tqdm(enumerate(train_dl)):\n",
    "  imgs = list(img.to(device) for img in images)\n",
    "  neural_images = torch.stack(imgs)\n",
    "  x = neural_images.to(device)\n",
    "  annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "  y = annotations[0]['labels'].to(device)\n",
    "  z = annotations[0]['boxes'].to(device)\n",
    "  data = x\n",
    "  targets = y\n",
    "  print(targets)\n",
    "  if batch == 1:\n",
    "    break\n",
    "\n",
    "# data = data.to(device)\n",
    "# targets = targets.to(device)\n",
    "print(data.shape)\n",
    "\n",
    "spk_rec, mem_rec = model(data)\n",
    "print(mem_rec.size())\n",
    "\n",
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# sum loss at every step\n",
    "for step in range(num_steps):\n",
    "  loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "\n",
    "print_batch_accuracy(data, targets, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SF.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(train_loader, net, num_steps):\n",
    "  with torch.no_grad():\n",
    "    total = 0\n",
    "    acc = 0\n",
    "    net.eval()\n",
    "\n",
    "    train_loader = iter(train_loader)\n",
    "    for data, targets in train_loader:\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "      spk_rec, _ = forward_pass(net, num_steps, data)\n",
    "\n",
    "      acc += SF.accuracy_rate(spk_rec, targets) * spk_rec.size(1)\n",
    "      total += spk_rec.size(1)\n",
    "\n",
    "  return acc/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # Defining the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, betas=(0.9, 0.999))\n",
    "    num_of_epochs = 10\n",
    "    epochs = []\n",
    "    losses = []\n",
    "    test_acc_hist = []\n",
    "    get_num_correct = []\n",
    "    # Creating a directory for storing models\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        tot_loss = 0\n",
    "        tot_correct = 0\n",
    "        train_start = time.time()\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        for batch, (images, annotations) in tqdm(enumerate(train_dl)):\n",
    "\n",
    "        \t# Converting data from cpu to GPU if available to improve speed\n",
    "            imgs = list(img.to(device) for img in images)\n",
    "            neural_images = torch.stack(imgs)\n",
    "            x = neural_images.to(device)\n",
    "            annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "            y = annotations[0]['labels'].to(device)\n",
    "            z = annotations[0]['boxes'].to(device)\n",
    "            avg_loss = backprop.BPTT(model, train_loader, optimizer=optimizer, criterion=loss_fn,\n",
    "                            num_steps=num_steps, time_var=False, device=device)\n",
    "            # Sets the gradients of all optimized tensors to zero\n",
    "            optimizer.zero_grad()\n",
    "            [y_pred,z_pred]= model(x)\n",
    "            # Compute loss (here CrossEntropyLoss)\n",
    "            class_loss = F.cross_entropy(y_pred, y)\n",
    "            box_loss = F.mse_loss(z_pred, z)\n",
    "            (box_loss + class_loss).backward()\n",
    "            # class_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Train batch:\", batch+1, \" epoch: \", epoch, \" \",\n",
    "                  (time.time()-train_start)/60, end='\\r')\n",
    "\n",
    "        model.eval()\n",
    "        for batch, (images, annotations) in tqdm(enumerate(valid_dl)):\n",
    "        \t# Converting data from cpu to GPU if available to improve speed\t\n",
    "            imgs = list(img.to(device) for img in images)\n",
    "            neural_images = torch.stack(imgs)\n",
    "            x = neural_images.to(device)\n",
    "            annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "            y = annotations[0]['labels'].to(device)\n",
    "            z = annotations[0]['boxes'].to(device)\n",
    "            # Sets the gradients of all optimized tensors to zero\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                [y_pred,z_pred]= model(x)\n",
    "                \n",
    "                # Compute loss (here CrossEntropyLoss)\n",
    "                class_loss = F.cross_entropy(y_pred, y)\n",
    "                box_loss = F.mse_loss(z_pred, z)\n",
    "                # Compute loss (here CrossEntropyLoss)\n",
    "\n",
    "            tot_loss += (class_loss.item() + box_loss.item())\n",
    "            tot_correct += get_num_correct(y_pred, y)\n",
    "            print(\"Test batch:\", batch+1, \" epoch: \", epoch, \" \",\n",
    "                  (time.time()-train_start)/60, end='\\r')\n",
    "        epochs.append(epoch)\n",
    "        losses.append(tot_loss)\n",
    "        print(\"Epoch\", epoch, \"Accuracy\", (tot_correct)/2.4, \"loss:\",\n",
    "              tot_loss, \" time: \", (time.time()-train_start)/60, \" mins\")\n",
    "        torch.save(model.state_dict(), \"model_ep\"+str(epoch+1)+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62d0206b570d77ef1bfa49bb04057592ebbd6080402d9290ba5af22960e1b27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
