{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='/media/gamedisk/COCO_dataset'\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco=COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 8 conv layers\n",
    "- 8 spiking (lif) layers\n",
    "\n",
    "- architecture similar to U-Net\n",
    "\n",
    "input -> LIF(conv) (5x5x32)? -> lif -> conv2\n",
    "\n",
    "encoder(model) -> decoder(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import snntorch as snn\n",
    "import numpy as np\n",
    "import random as r\n",
    "import torchvision\n",
    "from snntorch import surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "alpha = 0.9\n",
    "beta = 0.85\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 12, 4)\n",
    "        self.lif1 = snn.Leaky(beta = beta)\n",
    "        self.conv2 = nn.Conv2d(12, 64, 5)\n",
    "        self.lif2 = snn.Leaky(beta = beta)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk3_rec = []\n",
    "        mem3_rec = []\n",
    "\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.conv1(x)\n",
    "            spk1, mem1 = self.lif1(cur1,mem1)\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            print(spk2.shape)\n",
    "            cur3 = self.fc1(spk2.view(1600, -1))\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "            mem3_rec.append(mem3)\n",
    "\n",
    "        return torch.stack(spk3_rec), torch.stack(mem3_rec)        \n",
    "\n",
    "class Encode(nn.Module):\n",
    "    def __init__(self, channels  = (1, 2, 3, 16)):\n",
    "        super().__init__()\n",
    "        self.enc_models = nn.ModuleList([Model(channels[k], channels[k+1]) for k in range(len(channels)-1)])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, a):\n",
    "        features = [] \n",
    "        for model in self.enc_models:\n",
    "            a = model(a)\n",
    "            features.append(a)\n",
    "            print(a[1].shape)\n",
    "            a = self.pool(a[0])\n",
    "        return features\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=(16, 3, 2, 1)):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(channels[k], channels[k+1], 3, 3) for k in range(len(channels)-1)])\n",
    "        self.dec_models = nn.ModuleList([Model(channels[k], channels[k+1]) for k in range(len(channels)-1)])\n",
    "    \n",
    "    def forward(self, a, encodedata_features):\n",
    "        for k in range(len(self.channels)-1):\n",
    "            a = self.upconvs[k](a)\n",
    "            enc_features = self.crop(encodedata_features[k], a)\n",
    "            a = torch.cat([a, enc_features], dim=1)\n",
    "            a = self.dec_models[k](a)\n",
    "        return a\n",
    "\n",
    "    def crop(self, enc_features, a):\n",
    "        _, _, H, W = a.shape\n",
    "        enc_features = torchvision.transforms.CenterCrop([H, W])(enc_features)\n",
    "        return enc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1600x1 and 1600x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000007?line=2'>3</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m12\u001b[39m, \u001b[39m12\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000007?line=3'>4</a>\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000007?line=4'>5</a>\u001b[0m features \u001b[39m=\u001b[39m encodedata(a)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000007?line=5'>6</a>\u001b[0m decoder(a, features[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:])\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb Cell 8'\u001b[0m in \u001b[0;36mEncode.forward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=43'>44</a>\u001b[0m features \u001b[39m=\u001b[39m [] \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_models:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=45'>46</a>\u001b[0m     a \u001b[39m=\u001b[39m model(a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=46'>47</a>\u001b[0m     features\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=47'>48</a>\u001b[0m     \u001b[39mprint\u001b[39m(a[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb Cell 8'\u001b[0m in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=26'>27</a>\u001b[0m spk2, mem2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlif2(cur2, mem2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(spk2\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=28'>29</a>\u001b[0m cur3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(spk2\u001b[39m.\u001b[39;49mview(\u001b[39m1600\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=29'>30</a>\u001b[0m spk3, mem3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlif3(cur3, mem3)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arsalikhov/Documents/PSYCH420_final_project/code/model.ipynb#ch0000006?line=31'>32</a>\u001b[0m spk3_rec\u001b[39m.\u001b[39mappend(spk3)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/arsalikhov/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1600x1 and 1600x1)"
     ]
    }
   ],
   "source": [
    "decoder = Decoder().to(device)\n",
    "encodedata = Encode().to(device)\n",
    "a = torch.randn(1, 1, 12, 12)\n",
    "a = a.to(device)\n",
    "features = encodedata(a)\n",
    "decoder(a, features[::-1][1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b62d0206b570d77ef1bfa49bb04057592ebbd6080402d9290ba5af22960e1b27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
